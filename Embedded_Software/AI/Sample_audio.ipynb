{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNL3jLbksuRKzlpNj7uVdpz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"7UPvIsFmB6cR"},"outputs":[],"source":["# Upgrade environment to support TF 2.10 in Colab\n","!pip install -U --pre tensorflow tensorflow_datasets\n","!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"]},{"cell_type":"code","source":["import os\n","import pathlib\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from IPython import display\n","from scipy.signal import resample\n","\n","# Set the seed value for experiment reproducibility.\n","seed = 195397\n","tf.random.set_seed(seed)\n","np.random.seed(seed)"],"metadata":{"id":"sMSyACCDCMmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_PATH = 'data/mini_speech_commands'\n","\n","data_dir = pathlib.Path(DATASET_PATH)\n","if not data_dir.exists():\n","  tf.keras.utils.get_file(\n","      'mini_speech_commands.zip',\n","      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n","      extract=True,\n","      cache_dir='.', cache_subdir='data')"],"metadata":{"id":"wC62OURiCzHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n","commands = commands[commands != 'README.md']\n","print('Commands:', commands)"],"metadata":{"id":"1f1Xz35CCup_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n","    directory=data_dir,\n","    batch_size=64,\n","    validation_split=0.2,\n","    seed=0,\n","    output_sequence_length=16000,\n","    subset='both')\n","\n","label_names = np.array(train_ds.class_names)\n","print()\n","print(\"label names:\", label_names)"],"metadata":{"id":"qR3XDq28DGIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds.element_spec"],"metadata":{"id":"ZwkatUTJDG2G","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def squeeze(audio, labels):\n","  audio = tf.squeeze(audio, axis=-1)\n","  return audio, labels\n","\n","train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n","val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"],"metadata":{"id":"i0UwhXoADLbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_ds = val_ds.shard(num_shards=2, index=0)\n","val_ds = val_ds.shard(num_shards=2, index=1)"],"metadata":{"id":"dQooonswDNGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for example_audio, example_labels in train_ds.take(1):\n","  print(example_audio.shape)\n","  print(example_labels.shape)"],"metadata":{"id":"x4gWiI8hDQEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_names[[1,2,3,0]]"],"metadata":{"id":"VKLhDTb2DQ3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rows = 3\n","cols = 3\n","n = rows * cols\n","fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n","\n","for i in range(n):\n","  if i>=n:\n","    break\n","  r = i // cols\n","  c = i % cols\n","  ax = axes[r][c]\n","  ax.plot(example_audio[i].numpy())\n","  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n","  label = label_names[example_labels[i]]\n","  ax.set_title(label)\n","  ax.set_ylim([-1.1,1.1])\n","\n","plt.show()"],"metadata":{"id":"v5NBTULFDSKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_spectrogram(waveform):\n","  # Convert the waveform to a spectrogram via a STFT.\n","  spectrogram = tf.signal.stft(\n","      waveform, frame_length=255, frame_step=128)\n","  # Obtain the magnitude of the STFT.\n","  spectrogram = tf.abs(spectrogram)\n","  # Add a `channels` dimension, so that the spectrogram can be used\n","  # as image-like input data with convolution layers (which expect\n","  # shape (`batch_size`, `height`, `width`, `channels`).\n","  spectrogram = spectrogram[..., tf.newaxis]\n","  return spectrogram"],"metadata":{"id":"iCziMiZADTMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(3):\n","  label = label_names[example_labels[i]]\n","  waveform = example_audio[i]\n","  spectrogram = get_spectrogram(waveform)\n","\n","  print('Label:', label)\n","  print('Waveform shape:', waveform.shape)\n","  print('Spectrogram shape:', spectrogram.shape)\n","  print('Audio playback')\n","  display.display(display.Audio(waveform, rate=16000))"],"metadata":{"id":"dDjloL_eDcL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_spectrogram(spectrogram, ax):\n","  if len(spectrogram.shape) > 2:\n","    assert len(spectrogram.shape) == 3\n","    spectrogram = np.squeeze(spectrogram, axis=-1)\n","  # Convert the frequencies to log scale and transpose, so that the time is\n","  # represented on the x-axis (columns).\n","  # Add an epsilon to avoid taking a log of zero.\n","  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n","  height = log_spec.shape[0]\n","  width = log_spec.shape[1]\n","  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n","  Y = range(height)\n","  ax.pcolormesh(X, Y, log_spec)"],"metadata":{"id":"7aMyUzsiDfqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(2, figsize=(12, 8))\n","timescale = np.arange(waveform.shape[0])\n","axes[0].plot(timescale, waveform.numpy())\n","axes[0].set_title('Waveform')\n","axes[0].set_xlim([0, 16000])\n","\n","plot_spectrogram(spectrogram.numpy(), axes[1])\n","axes[1].set_title('Spectrogram')\n","plt.suptitle(label.title())\n","plt.show()"],"metadata":{"id":"fFjhVVNIDis3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_spec_ds(ds):\n","  return ds.map(\n","      map_func=lambda audio,label: (get_spectrogram(audio), label),\n","      num_parallel_calls=tf.data.AUTOTUNE)"],"metadata":{"id":"6Rs_XZP7DjyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_spectrogram_ds = make_spec_ds(train_ds)\n","val_spectrogram_ds = make_spec_ds(val_ds)\n","test_spectrogram_ds = make_spec_ds(test_ds)"],"metadata":{"id":"OGGUiWa1Dk-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n","  break"],"metadata":{"id":"uOfKw_lODl3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rows = 4\n","cols = 4\n","n = rows*cols\n","fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n","\n","for i in range(n):\n","    r = i // cols\n","    c = i % cols\n","    ax = axes[r][c]\n","    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n","    ax.set_title(commands[example_spect_labels[i].numpy()])\n","\n","plt.show()"],"metadata":{"id":"z9BY0L2JDmxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n","val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n","test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"Wrp8AiqwDn9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_shape = example_spectrograms.shape[1:]\n","print('Input shape:', input_shape)\n","num_labels = len(commands)\n","\n","# Instantiate the `tf.keras.layers.Normalization` layer.\n","norm_layer = layers.Normalization()\n","# Fit the state of the layer to the spectrograms\n","# with `Normalization.adapt`.\n","norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n","\n","model = models.Sequential([\n","    layers.Input(shape=input_shape),\n","    # Downsample the input.\n","    layers.Resizing(32, 32),\n","    # Normalize.\n","    norm_layer,\n","    layers.Conv2D(32, 3, activation='relu'),\n","    layers.Conv2D(64, 3, activation='relu'),\n","    layers.MaxPooling2D(),\n","    layers.Dropout(0.25),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.2),\n","    layers.Dense(num_labels),\n","])\n","\n","model.summary()"],"metadata":{"id":"3SsfTuBxDqFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy'],\n",")"],"metadata":{"id":"KEm1MIT3Drhi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 30\n","history = model.fit(\n","    train_spectrogram_ds,\n","    validation_data=val_spectrogram_ds,\n","    epochs=EPOCHS,\n","    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=10, min_delta=0.001),\n",")"],"metadata":{"id":"piNJ6LAyDvoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = history.history\n","plt.figure(figsize=(16,6))\n","plt.subplot(1,2,1)\n","plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n","plt.legend(['loss', 'val_loss'])\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss [CrossEntropy]')\n","\n","plt.subplot(1,2,2)\n","plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n","plt.legend(['accuracy', 'val_accuracy'])\n","plt.ylim([0, 100])\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy [%]')"],"metadata":{"id":"gmqV_tHtDwtL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(test_spectrogram_ds, return_dict=True)"],"metadata":{"id":"VlfYn0J8D-Tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = model.predict(test_spectrogram_ds)"],"metadata":{"id":"Dzu3IKKxD5xT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = tf.argmax(y_pred, axis=1)"],"metadata":{"id":"19G14lIDEBDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"],"metadata":{"id":"1h3VEesJECVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(confusion_mtx,\n","            xticklabels=commands,\n","            yticklabels=commands,\n","            annot=True, fmt='g')\n","plt.xlabel('Prediction')\n","plt.ylabel('Label')\n","plt.show()"],"metadata":{"id":"k3wQmUKuEDQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. 오디오 파일 읽기\n","def load_audio(audio_path):\n","    audio_binary = tf.io.read_file(str(audio_path))\n","    waveform, sample_rate = tf.audio.decode_wav(audio_binary, desired_channels=1)\n","    waveform = tf.squeeze(waveform, axis=-1)  # 채널 제거\n","    return waveform, sample_rate\n","\n","# 2. 오디오 리샘플링\n","def resample_audio(waveform, original_sample_rate, desired_sample_rate=16000):\n","    if original_sample_rate.numpy() != desired_sample_rate:\n","        original_samples = waveform.shape[0]\n","        desired_samples = int(original_samples * desired_sample_rate / original_sample_rate.numpy())\n","        waveform = resample(waveform.numpy(), desired_samples)\n","    return waveform, desired_sample_rate\n","\n","# 5. 메인 실행 코드\n","audio_path = data_dir/'left.wav'  # 업로드한 오디오 파일 경로\n","\n","# 원본 오디오\n","original_waveform, original_sample_rate = load_audio(audio_path)\n","\n","# 리샘플링된 오디오\n","resampled_waveform, resampled_sample_rate = resample_audio(original_waveform, original_sample_rate)\n","\n","# 스펙트로그램 생성\n","original_spectrogram = get_spectrogram(original_waveform)\n","resampled_spectrogram = get_spectrogram(resampled_waveform)\n","\n","# 시각화\n","fig, axes = plt.subplots(4, figsize=(12, 16))\n","\n","# 1. 원본 파형\n","axes[0].plot(np.arange(original_waveform.shape[0]), original_waveform.numpy())\n","axes[0].set_title(f'Original Waveform (Sample rate: {original_sample_rate.numpy()} Hz)')\n","axes[0].set_xlabel('Sample')\n","axes[0].set_ylabel('Amplitude')\n","\n","# 2. 리샘플링된 파형\n","axes[1].plot(np.arange(len(resampled_waveform)), resampled_waveform)\n","axes[1].set_title(f'Resampled Waveform (Sample rate: {resampled_sample_rate} Hz)')\n","axes[1].set_xlabel('Sample')\n","axes[1].set_ylabel('Amplitude')\n","\n","# 3. 원본 스펙트로그램\n","plot_spectrogram(original_spectrogram.numpy(), axes[2])\n","axes[2].set_title('Original Spectrogram')\n","\n","# 4. 리샘플링된 스펙트로그램\n","plot_spectrogram(resampled_spectrogram.numpy(), axes[3])\n","axes[3].set_title('Resampled Spectrogram')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","display.display(display.Audio(resampled_waveform, rate=16000))"],"metadata":{"id":"VK4btb9w5SIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 리샘플링된 파형을 TensorFlow 텐서로 변환\n","resampled_waveform_tensor = tf.convert_to_tensor(resampled_waveform, dtype=tf.float32)\n","\n","# 스펙트로그램 생성\n","resampled_spectrogram = get_spectrogram(resampled_waveform_tensor)\n","\n","# 모델 입력 형식에 맞게 차원 확장 (배치 차원 추가)\n","resampled_input = resampled_spectrogram[tf.newaxis, ...]\n","\n","# 모델 예측\n","prediction = model(resampled_input)\n","\n","# 예측 결과 시각화\n","plt.bar(commands, tf.nn.softmax(prediction[0]))\n","plt.title('Predicted Command')\n","plt.show()\n","\n","# 오디오 재생\n","display.display(display.Audio(resampled_waveform, rate=resampled_sample_rate))\n"],"metadata":{"id":"L8PPPvjkQvhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = data_dir/'left.wav'\n","x = tf.io.read_file(str(x))\n","x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=48000,)\n","x = tf.squeeze(x, axis=-1)\n","waveform = x\n","x = get_spectrogram(x)\n","x = x[tf.newaxis,...]\n","\n","prediction = model(x)\n","plt.bar(commands, tf.nn.softmax(prediction[0]))\n","plt.title('No')\n","plt.show()\n","\n","display.display(display.Audio(waveform, rate=48000))"],"metadata":{"id":"ayejt0Jk6N7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = data_dir / 'right.wav'\n","x = tf.io.read_file(str(x))\n","x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=48000)\n","x = tf.squeeze(x, axis=-1)\n","waveform = x\n","x = get_spectrogram(x)\n","x = x[tf.newaxis, ...]\n","\n","prediction = model(x)\n","\n","# 그래프 크기 조정\n","plt.figure(figsize=(12, 6))  # 너비 12, 높이 6으로 설정\n","\n","# 바 그래프 시각화\n","plt.bar(commands, tf.nn.softmax(prediction[0]))\n","plt.title('right')\n","\n","# 가로축 레이블 회전 추가\n","plt.xticks(rotation=45)  # 45도 회전\n","plt.xlabel('Commands')  # x축 레이블 추가\n","plt.ylabel('Probability')  # y축 레이블 추가\n","plt.tight_layout()  # 그래프 간격 조정\n","\n","plt.show()\n","\n","# 오디오 재생\n","display.display(display.Audio(waveform, rate=48000))\n"],"metadata":{"id":"uDH6ryYXO0gm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_LBrpZojYUhy"},"execution_count":null,"outputs":[]}]}