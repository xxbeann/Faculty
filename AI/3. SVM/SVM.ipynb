{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#수학연산과 배열처리\n",
    "import numpy as np\n",
    "#데이터 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "#데이터셋 생성 모듈\n",
    "from sklearn.datasets import make_moons, make_classification, make_blobs\n",
    "#SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "#분류 모델이 생성한 결정 경계를 시각화하는 기능 \n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "#학습집합과 테스트 집합으로 분리\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Separable Data\n",
    "# categorical type_ex) 개, 고양이\n",
    "\n",
    "# 선형 분리 데이터 생성\n",
    "X, y = make_blobs(\n",
    "    n_samples=200,# 데이터 포인트 총 수\n",
    "    n_features=2, # 특성의 수\n",
    "    centers=2, # 두 개의 클래스\n",
    "    cluster_std=1, # 클러스터 표준편차\n",
    "    random_state=195397 # 본인학번\n",
    ")\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.5, # 전체 데이터의 50%를 테스트 집합으로 사용.\n",
    "    random_state=0 # 난수 생성 시드 값 0으로 고정\n",
    ") \n",
    "# 데이터 시각화\n",
    "print(\"X shape:\", X.shape) #(총 수, 특성 수)\n",
    "print(\"y:\", y) # 0과 1로 classification\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.bwr, s=80, edgecolor='k') # train_검정 테두리\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.bwr, s=80, edgecolor='cyan') # test_파란 테두리\n",
    "'''\n",
    "빨간색: y_train 또는 y_test에서 값이 1인 데이터 포인트들\n",
    "파란색: y_train 또는 y_test에서 값이 0인 데이터 포인트들\n",
    "cmap=plt.cm.bwr를 사용하면, 0의 레이블을 가진 데이터는 파란색 계열로\n",
    "1의 레이블을 가진 데이터는 빨간색 계열로 표현된다.\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC( #SVM classifier -> SVC\n",
    "    C = 1e0,  # cost_slack variable\n",
    "    kernel = 'linear', \n",
    "    tol=1e-3,  # 학습 종료 기준\n",
    "    max_iter=-1, # 최대 반복 횟수, -1 제한 X\n",
    "    decision_function_shape='ovr', # 결정 함수 ovr\n",
    "    random_state=0, # 시드는 0으로 고정\n",
    ")\n",
    "\n",
    "#gradient로 학습\n",
    "model.fit(X_train,y_train) #학습\n",
    "pred_train = model.predict(X_train)\n",
    "\n",
    "print(\"n iter:\", model.n_iter_) # 반복 횟수\n",
    "print(\"n support:\", model.n_support_) # 속성\n",
    "\n",
    "# 결정 경계 및 서포트 벡터 시각화\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            model, X_train, grid_resolution=100, response_method=\"predict\", cmap=plt.cm.coolwarm, alpha=0.8, eps=0.5\n",
    ")\n",
    "\n",
    "#초록색 테두리가 서포트 벡터(train_set에 있는 것만 쓴다.)\n",
    "plt.scatter(X_train[model.support_,0], X_train[model.support_,1], s=180, edgecolor='g')\n",
    "# 학습 데이터를 실제 레이블(y_train)에 따라 표현\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.bwr, s=80, edgecolor='k')\n",
    "# 학습 데이터를 모델 예측에 따라 표현 (pred_train)\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=pred_train, cmap=plt.cm.bwr, s=10)\n",
    "plt.show()\n",
    "\n",
    "# 정확도 계산\n",
    "print(\"train acc:\", np.sum(pred_train==y_train) / len(y_train) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 검증, 예측 수행\n",
    "pred_test = model.predict(X_test)\n",
    "# 테스트 정확도 계산\n",
    "print(\"test acc:\", np.sum(pred_test==y_test) / len(y_test) )\n",
    "# 결정 경계 시각화\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            model, X_train, grid_resolution=100, response_method=\"predict\", cmap=plt.cm.coolwarm, alpha=0.8, eps=0.5\n",
    ")\n",
    "# 실제 레이블_청록색 테두리\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.bwr, s=80, edgecolor='cyan')\n",
    "# 예측 데이터 but 더 작은 점\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=pred_test, cmap=plt.cm.bwr, s=10)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moon Data(실습용)\n",
    "X, y = make_moons(n_samples=200,\n",
    "                noise=0.2, \n",
    "                shuffle=True, \n",
    "                random_state=195397 #본인학번이용\n",
    "                )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.5, \n",
    "    random_state=0 # 0으로 고정\n",
    ") \n",
    "\n",
    "\n",
    "print(\"X shape:\", X.shape) # (총 수, 특성 수)\n",
    "print(\"y:\", y)\n",
    "# 학습 데이터(train_set) 검은색 테두리\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.bwr, s=80, edgecolor='k')\n",
    "# 테스트 데이터(test_set) 청록색 테두리\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.bwr, s=80, edgecolor='cyan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Poly 커널과, RBF 커널로 / 파라미터를 변경해 가며 결과 분석\n",
    "안과 밖을 나누는데 poly커널로는 차수가 커진다.\n",
    "그래서 rbf커널로 해보자! -> 3d로 그려짐\n",
    "c랑 감마값, c랑 디그리 조정해서 분석~\n",
    "train은 테두리가 검정색이고 test는 파란색\n",
    "'''\n",
    "1e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC( #SVM classifier -> SVC\n",
    "    C = 1e1,\n",
    "    kernel = 'poly',\n",
    "    degree=3,\n",
    "    tol=1e-3,\n",
    "    max_iter=-1,\n",
    "    decision_function_shape='ovr',\n",
    "    random_state=0, # 시드는 0으로 고정\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train) #학습\n",
    "pred_train = model.predict(X_train)\n",
    "\n",
    "print(\"n iter:\", model.n_iter_)\n",
    "print(\"n support:\", model.n_support_)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            model, X_train, grid_resolution=100, response_method=\"predict\", cmap=plt.cm.coolwarm, alpha=0.8, eps=0.5\n",
    ")\n",
    "plt.scatter(X_train[model.support_,0], X_train[model.support_,1], s=180, edgecolor='g')\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.bwr, s=80, edgecolor='k')\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=pred_train, cmap=plt.cm.bwr, s=10)\n",
    "plt.show()\n",
    "\n",
    "print(\"train acc:\", np.sum(pred_train==y_train) / len(y_train) )\n",
    "#gradient로 학습\n",
    "#초록색 테두리가 서포트 (train_set에 있는 것만 쓴다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test) #test 검증\n",
    "print(\"test acc:\", np.sum(pred_test==y_test) / len(y_test) )\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            model, X_train, grid_resolution=100, response_method=\"predict\", cmap=plt.cm.coolwarm, alpha=0.8, eps=0.5\n",
    ")\n",
    "\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.bwr, s=80, edgecolor='cyan')\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=pred_test, cmap=plt.cm.bwr, s=10)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC( #SVM classifier -> SVC\n",
    "    C = 10,\n",
    "    kernel = 'rbf', # rbf는 degree가 아니라 감마라는 파라미터가 중요하다. 그래프가 얼마나 표족하냐 완만하냐를 결정\n",
    "    gamma=10,\n",
    "    tol=1e-3,\n",
    "    max_iter=-1,\n",
    "    decision_function_shape='ovr',\n",
    "    random_state=0, # 시드는 0으로 고정\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train) #학습\n",
    "pred_train = model.predict(X_train)\n",
    "\n",
    "print(\"n iter:\", model.n_iter_)\n",
    "print(\"n support:\", model.n_support_)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            model, X_train, grid_resolution=100, response_method=\"predict\", cmap=plt.cm.coolwarm, alpha=0.8, eps=0.5\n",
    ")\n",
    "plt.scatter(X_train[model.support_,0], X_train[model.support_,1], s=180, edgecolor='g')\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.bwr, s=80, edgecolor='k')\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=pred_train, cmap=plt.cm.bwr, s=10)\n",
    "plt.show()\n",
    "\n",
    "print(\"train acc:\", np.sum(pred_train==y_train) / len(y_train) )\n",
    "#gradient로 학습\n",
    "#초록색 테두리가 서포트 (train_set에 있는 것만 쓴다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test) #test 검증\n",
    "print(\"test acc:\", np.sum(pred_test==y_test) / len(y_test) )\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "            model, X_train, grid_resolution=100, response_method=\"predict\", cmap=plt.cm.coolwarm, alpha=0.8, eps=0.5\n",
    ")\n",
    "\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.bwr, s=80, edgecolor='cyan')\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=pred_test, cmap=plt.cm.bwr, s=10)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
